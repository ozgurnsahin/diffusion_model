{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a1c0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_fidelity import calculate_metrics\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbd785e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35d24722",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_datasets.pkl', 'rb') as f:\n",
    "    processed_datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a765880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "def load_trained_model(model_path, device):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    model_components = checkpoint['model_components']\n",
    "    noise_schedule = checkpoint['noise_schedule']\n",
    "    training_info = checkpoint['training_info']\n",
    "    \n",
    "    for name, component in model_components.items():\n",
    "        component.to(device)\n",
    "        component.eval()\n",
    "    \n",
    "    print(f\"Category: {training_info['category']}\")\n",
    "    print(f\"Training epochs: {training_info['num_epochs']}\")\n",
    "    print(f\"Final loss: {training_info['final_loss']:.4f}\")\n",
    "    print(f\"Model components: {list(model_components.keys())}\")\n",
    "    \n",
    "    return model_components, noise_schedule, training_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8be298b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate sketches\n",
    "def model_forward_training(sequences, model_components, categories, timesteps):\n",
    "    # Get components\n",
    "    stroke_embedder = model_components['stroke_embedder']\n",
    "    category_embedder = model_components['category_embedder']\n",
    "    temporal_encoder = model_components['temporal_encoder']\n",
    "    noise_predictor = model_components['noise_predictor']\n",
    "    attention_layer = model_components['attention_layer']\n",
    "    \n",
    "    batch_size, seq_len, _ = sequences.shape\n",
    "    \n",
    "    # Embded stroke sequences\n",
    "    stroke_embeddings = stroke_embedder(sequences) # [batch, seq_len, embedding_dim]\n",
    "    \n",
    "    # Embed categories and inject into sequence\n",
    "    category_embeddings = category_embedder(categories)  # [batch, embedding_dim]\n",
    "    category_expanded = category_embeddings.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "    \n",
    "    # Combine stroke and category embeddings\n",
    "    conditioned_embeddings = stroke_embeddings + category_expanded\n",
    "    \n",
    "    # Process through LSTM\n",
    "    lstm_output, _ = temporal_encoder(conditioned_embeddings)  # [batch, seq_len, hidden_dim]\n",
    "    \n",
    "    # Self attention\n",
    "    final_output, _ = attention_layer(lstm_output, lstm_output, lstm_output)\n",
    "    \n",
    "    # Predict noise\n",
    "    predicted_noise = noise_predictor(lstm_output)\n",
    "    \n",
    "    return predicted_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39c8d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from the model\n",
    "def generate_samples(model_components, num_samples, seq_length, device):\n",
    "    generated_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Start with random noise\n",
    "            sketch = torch.randn(1, seq_length, 3, device=device)\n",
    "            sketch[0, :, 2] = 1.0  # Start with pen down\n",
    "            \n",
    "            # Category for conditioning\n",
    "            category = torch.tensor([0], device=device)\n",
    "            \n",
    "            # Denoising loop\n",
    "            num_steps = 1000\n",
    "            for step in range(num_steps):\n",
    "                t = torch.tensor([num_steps - step - 1], device=device)\n",
    "                predicted_noise = model_forward_training(sketch, model_components, category, t)\n",
    "                \n",
    "                # Denoising step\n",
    "                alpha_t = 1.0 - 0.02 * step / num_steps\n",
    "                sketch[:, :, :2] = sketch[:, :, :2] - alpha_t * predicted_noise[:, :, :2]\n",
    "                sketch[:, :, 2] = torch.clamp(sketch[:, :, 2], 0, 3)\n",
    "            \n",
    "            generated_samples.append(sketch[0].cpu().numpy())\n",
    "            \n",
    "            if (i + 1) % 25 == 0:\n",
    "                print(f\"  Progress: {i + 1}/{num_samples}\")\n",
    "    return np.array(generated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0e10af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stroke_to_image_tensor(stroke_sequence, image_size=64):\n",
    "    img = np.zeros((image_size, image_size))\n",
    "    \n",
    "    for i in range(len(stroke_sequence)):\n",
    "        x, y, pen_state = stroke_sequence[i]\n",
    "        \n",
    "        # Skip padding and end markers\n",
    "        if pen_state == -1 or pen_state == 3:\n",
    "            continue\n",
    "            \n",
    "        # Only draw when pen is down (pen_state == 2)\n",
    "        if pen_state == 2:\n",
    "            # Convert from [-1, 1] to [0, image_size-1]\n",
    "            x_pixel = int((x + 1) * (image_size - 1) / 2)\n",
    "            y_pixel = int((y + 1) * (image_size - 1) / 2)\n",
    "            \n",
    "            x_pixel = np.clip(x_pixel, 0, image_size-1)\n",
    "            y_pixel = np.clip(y_pixel, 0, image_size-1)\n",
    "            \n",
    "            # Draw point with small brush\n",
    "            for dx in [-1, 0, 1]:\n",
    "                for dy in [-1, 0, 1]:\n",
    "                    nx, ny = x_pixel + dx, y_pixel + dy\n",
    "                    if 0 <= nx < image_size and 0 <= ny < image_size:\n",
    "                        img[ny, nx] = 1.0\n",
    "    return img\n",
    "\n",
    "def convert_to_tensors(stroke_data, image_size=64):\n",
    "    images = []\n",
    "    for i, stroke_seq in enumerate(stroke_data):\n",
    "        img = stroke_to_image_tensor(stroke_seq, image_size)\n",
    "        # Convert to RGB format: (3, H, W)\n",
    "        img_rgb = np.stack([img, img, img], axis=0)\n",
    "        images.append(img_rgb)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Converted {i + 1}/{len(stroke_data)} images\")\n",
    "    \n",
    "    # Convert to torch tensor: (N, 3, H, W)\n",
    "    tensor = torch.from_numpy(np.array(images)).float()\n",
    "    \n",
    "    # Normalize to [0, 1] range\n",
    "    tensor = tensor.clamp(0, 1)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6a259d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kid_fid_metrics(real_tensor, generated_tensor):\n",
    "    # Ensure tensors are in correct format and range\n",
    "    real_tensor = real_tensor.clamp(0, 1)\n",
    "    generated_tensor = generated_tensor.clamp(0, 1)\n",
    "    \n",
    "    # Convert to uint8 format (0-255)\n",
    "    real_tensor = (real_tensor * 255).byte()\n",
    "    generated_tensor = (generated_tensor * 255).byte()\n",
    "    \n",
    "    print(f\"Real tensor shape: {real_tensor.shape}\")\n",
    "    print(f\"Generated tensor shape: {generated_tensor.shape}\")\n",
    "    \n",
    "    # Calculate metrics directly from tensors\n",
    "    metrics = calculate_metrics(\n",
    "        input1=generated_tensor,\n",
    "        input2=real_tensor,\n",
    "        fid=True,\n",
    "        kid=True,\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    fid_score = metrics['frechet_inception_distance']\n",
    "    kid_score = metrics['kernel_inception_distance_mean']\n",
    "    \n",
    "    return fid_score, kid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52ca3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "def run_evaluation(category, device, processed_datasets, model_path, num_samples=500):\n",
    "    # Load trained model\n",
    "    model_components, noise_schedule, training_info = load_trained_model(model_path, device)\n",
    "    \n",
    "    # Load test data\n",
    "    real_data = processed_datasets[category]['test_data']\n",
    "    seq_length = real_data.shape[1]\n",
    "    \n",
    "    print(f\"Using {num_samples} samples\")\n",
    "    # Use subset for faster evaluation\n",
    "    \n",
    "    num_samples = min(num_samples, len(real_data))\n",
    "    real_data_subset = real_data[:num_samples]\n",
    "    \n",
    "    generated_data = generate_samples(model_components, num_samples, seq_length, device)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    real_tensor = convert_to_tensors(real_data_subset)\n",
    "    generated_tensor = convert_to_tensors(generated_data)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fid_score, kid_score = calculate_kid_fid_metrics(real_tensor, generated_tensor)\n",
    "    \n",
    "    if fid_score is not None:\n",
    "        print(\"\\nRESULTS:\")\n",
    "        print(f\"FID Score: {fid_score:.4f}\")\n",
    "        print(f\"KID Score: {kid_score:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'category': category,\n",
    "            'FID': fid_score,\n",
    "            'KID': kid_score,\n",
    "            'num_samples': num_samples,\n",
    "            'seq_length': seq_length\n",
    "        }\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a95d80ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: bus\n",
      "Training epochs: 5\n",
      "Final loss: 0.0319\n",
      "Model components: ['stroke_embedder', 'category_embedder', 'temporal_encoder', 'noise_predictor']\n",
      "Using 500 samples\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m score_dict \u001b[38;5;241m=\u001b[39m \u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/sketch_diffusion_bus_20250802_144910.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 16\u001b[0m, in \u001b[0;36mrun_evaluation\u001b[0;34m(category, device, processed_datasets, model_path, num_samples)\u001b[0m\n\u001b[1;32m     13\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_samples, \u001b[38;5;28mlen\u001b[39m(real_data))\n\u001b[1;32m     14\u001b[0m real_data_subset \u001b[38;5;241m=\u001b[39m real_data[:num_samples]\n\u001b[0;32m---> 16\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Convert to tensors\u001b[39;00m\n\u001b[1;32m     19\u001b[0m real_tensor \u001b[38;5;241m=\u001b[39m convert_to_tensors(real_data_subset)\n",
      "Cell \u001b[0;32mIn[41], line 18\u001b[0m, in \u001b[0;36mgenerate_samples\u001b[0;34m(model_components, num_samples, seq_length, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[1;32m     17\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([num_steps \u001b[38;5;241m-\u001b[39m step \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 18\u001b[0m     predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43msketch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Denoising step\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     noise_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.02\u001b[39m \u001b[38;5;241m*\u001b[39m (num_steps \u001b[38;5;241m-\u001b[39m step) \u001b[38;5;241m/\u001b[39m num_steps\n",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m, in \u001b[0;36mmodel_forward_training\u001b[0;34m(sequences, model_components, categories, timesteps)\u001b[0m\n\u001b[1;32m      9\u001b[0m batch_size, seq_len, _ \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Embded stroke sequences\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m stroke_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mstroke_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [batch, seq_len, embedding_dim]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Embed categories and inject into sequence\u001b[39;00m\n\u001b[1;32m     15\u001b[0m category_embeddings \u001b[38;5;241m=\u001b[39m category_embedder(categories)  \u001b[38;5;66;03m# [batch, embedding_dim]\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "score_dict = run_evaluation('bus', device, processed_datasets, 'models/sketch_diffusion_bus_20250802_144910.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f8e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
